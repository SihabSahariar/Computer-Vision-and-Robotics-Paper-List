# IROS 2020

## Vision-based Robotic Grasping
### Indirect Object-to-Robot Pose Estimation from an External Monocular RGB Camera

*  标题：基于外部单目RGB摄像机的间接目标到机器人姿态估计
*  作者：NVIDIA

  We present a robotic grasping system that uses a single external monocular RGB camera as input. The object-torobot pose is computed indirectly by combining the output of two neural networks: one that estimates the object-to-camera pose, and another that estimates the robot-to-camera pose. Both networks are trained entirely on synthetic data, relying on domain randomization to bridge the sim-to-real gap. Because the latter network performs online camera calibration, the camera can be moved freely during execution without affecting the quality of the grasp. Experimental results analyze the effect of camera placement, image resolution, and pose refinement in the context of grasping several household objects. We also present results on a new set of 28 textured household toy grocery objects, which have been selected to be accessible to other researchers. To aid reproducibility of the research, we offer 3D scanned textured models, along with pre-trained weights for pose estimation. Video: https://youtu.be/E0J91llX-ys

  提出了一个机器人抓取系统，使用一个外部单目RGB相机作为输入。通过组合两个神经网络的输出间接计算目标机器人的姿势：一个用于估计目标到摄像机的姿势，另一个用于估计机器人到摄像机的姿势。这两个网络都完全基于合成数据进行训练，依靠领域随机化将sim连接到真实gap。由于后一个网络执行在线摄像机校准，因此在执行过程中摄像机可以自由移动，而不会影响抓取质量。实验结果分析了在抓取多个家用物体的情况下，相机位置、图像分辨率和姿势优化的效果。我们还介绍了一组新的28个纹理家用玩具杂物目标的结果，这些目标数据已开源供其他研究人员使用。为了帮助研究的再现性，我们提供了3D扫描纹理模型，以及用于姿势估计的预训练权重。

### PERCH 2.0: Fast and Accurate GPU-based Perception via Search for Object Pose Estimation

*  标题：PERCH 2.0: 基于GPU的快速准确感知,通过搜索进行目标位姿估计
*  作者：Carnegie Mellon University (CMU)

  Pose estimation of known objects is fundamental to tasks such as robotic grasping and manipulation. The need for reliable grasping imposes stringent accuracy requirements on pose estimation in cluttered, occluded scenes in dynamic environments. Modern methods employ large sets of training data to learn features in order to find correspondence between 3D models and observed data. However these methods require extensive annotation of ground truth poses. An alternative is to use algorithms that search for the best explanation of the observed scene in a space of possible rendered scenes. A recently developed algorithm, PERCH (PErception Via SeaRCH) does so by using depth data to converge to a globally optimum solution using a search over a specially constructed tree. While PERCH offers strong guarantees on accuracy, the current formulation suffers from low scalability owing to its high runtime. In addition, the sole reliance on depth data for pose estimation restricts the algorithm to scenes where no two objects have the same shape. In this work, we propose PERCH 2.0, a novel perception via search strategy that takes advantage of GPU acceleration and RGB data. We show that our approach can achieve a speedup of 100x over PERCH, as well as better accuracy than the state-of-the-art data-driven approaches on 6-DoF pose estimation without the need for annotating ground truth poses in the training data. Our code and video are available at https://sbpl-cruz.github.io/perception/.

  已知物体的姿态估计是机器人抓取和操纵等任务的基础。对可靠抓取的需求对动态环境中杂乱、遮挡场景中的姿态估计提出了严格的精度要求。现代方法使用大量的训练数据来学习特征，以便在三维模型和观测数据之间找到对应关系。然而，这些方法需要对地面真实姿态进行广泛的注释。另一种方法是使用算法，在可能渲染场景的空间中搜索观察场景的最佳解释。最近开发的一种算法PERCH（PErception-Via-SeaRCH）通过使用深度数据在一棵特殊构造的树上搜索，从而收敛到全局最优解。虽然PERCH在准确性方面提供了强有力的保证，但由于运行时间长，当前的公式存在可伸缩性差的问题。此外，仅依赖深度数据进行姿势估计将算法限制在没有两个目标具有相同形状的场景中。在这项工作中，我们提出了PERCH 2.0，这是一种利用GPU加速和RGB数据的新的搜索感知策略。我们表明，我们的方法可以实现100倍的加速比，并且在6自由度姿态估计上比最先进的数据驱动方法具有更好的精度，而无需在训练数据中注释地面真实姿态。代码和视频: https://sbpl-cruz.github.io/perception/.


### se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains

*  标题：se(3)-TrackNet: 通过校准合成域中的图像残差进行数据驱动的6D姿势跟踪
*  作者：Rutgers University

  Tracking the 6D pose of objects in video sequences is important for robot manipulation. This task, however, introduces multiple challenges: (i) robot manipulation involves significant occlusions; (ii) data and annotations are troublesome and difficult to collect for 6D poses, which complicates machine learning solutions, and (iii) incremental error drift often accumulates in long term tracking to necessitate re-initialization of the object’s pose. This work proposes a data-driven optimization approach for long-term, 6D pose tracking. It aims to identify the optimal relative pose given the current RGB-D observation and a synthetic image conditioned on the previous best estimate and the object’s model. The key contribution in this context is a novel neural network architecture, which appropriately disentangles the feature encoding to help reduce domain shift, and an effective 3D orientation representation via Lie Algebra. Consequently, even when the network is trained only with synthetic data can work effectively over real images.
Comprehensive experiments over benchmarks - existing ones as well as a new dataset with significant occlusions related to object manipulation - show that the proposed approach achieves consistently robust estimates and outperforms alternatives, even though they have been trained with real images. The approach is also the most computationally efficient among the alternatives and achieves a tracking frequency of 90.9Hz. Code, data and supplementary video for this project are available at https://github.com/wenbowen123/iros20-6d-pose-tracking

  跟踪视频序列中物体的6D姿态对于机器人操作非常重要。然而，这项任务带来了多重挑战：（i）机器人操作涉及严重的遮挡；（ii）对于6D姿势，数据和注释很麻烦且难以收集，这使机器学习解决方案变得复杂；（iii）长期跟踪过程中，增量误差漂移经常累积，因此需要重新初始化目标的姿态。这项工作提出了一种数据驱动的长期6D姿态跟踪优化方法。它的目的是在当前RGB-D观测和合成图像条件下，根据先前的最佳估计和目标模型，确定最佳相对姿态。这方面的关键贡献是一种新的神经网络结构，它适当地分离了特征编码以帮助减少域移位，并通过李代数实现了有效的三维方向表示。因此，即使仅使用合成数据训练网络，也可以在真实图像上有效工作。
在基准上进行的综合实验（现有的以及与目标操作相关的具有显著遮挡的新数据集）表明，所提出的方法实现了一致的稳健估计，并且优于其他方法，即使它们已经用真实图像进行了训练。该方法也是备选方案中计算效率最高的，可实现90.9Hz的跟踪频率。

## IROS 2020 Awarded Papers

### Best Paper Award	
* SwingBot: Learning Physical Features from In-Hand Tactile Exploration for Dynamic Swing-Up Manipulation
* Authors: Wang, Chen; Wang, Shaoxiong; Romero, Branden; Veiga,Filipe Fernandes; and Adelson, Edward
* Paper link: https://ras.papercept.net/proceedings/IROS20/1576.pdf

### Best Student Paper Award
* Computational Design of Balanced Open Link Planar Mechanisms with Counterweights from User Sketches
* Authors: Takahashi, Takuto; Okuno, Hiroshi G.; Sugano, Shigeki; Coros, Stelian; and Thomaszewski, Bernhard
* Paper link: https://ras.papercept.net/proceedings/IROS20/1425.pdf

### IROS Best Paper Award on Agri-Robotics Sponsored by YANMAR	
* Unsupervised Domain Adaptation for Transferring Plant Classification Systems to New Field Environments, Crops, and Robots
* Authors: Gogoll, Dario; Lottes, Philipp; Weyler, Jan; Petrinic, Nik; and Stachniss, Cyrill
* Paper link: https://ras.papercept.net/proceedings/IROS20/3252.pdf

### IROS Best Paper Award on Cognitive Robotics Sponsored by KROS	
* Representation and Experience-Based Learning of Explainable Models for Robot Action Execution
* Authors: Mitrevski, Alex; Plöger, Paul G.; and Lakemeyer, Gerhard
* Paper link: https://ras.papercept.net/proceedings/IROS20/3163.pdf

### IROS Best Paper Award on Robot Mechanisms and Design Sponsored by ROBOTIS	
* FreeBOT: A Freeform Modular Self-Reconfigurable Robot with Arbitrary Connection Point - Design and Implementation
* Authors: Liang, Guanqi; Luo, Haobo; Li, Ming; Qian, Huihuan; and Lam, Tin Lun 
* Paper link: https://ras.papercept.net/proceedings/IROS20/0857.pdf

### IROS Best Application Paper Award Sponsored by ICROS	
* MHYRO: Modular HYbrid RObot for Contact Inspection and Maintenance in Oil & Gas Plants
* Authors: López, Abraham; Sanchez-Cuevas, Pedro J; Suarez, Alejandro; Soldado, Ámbar; Ollero, Anibal; et al
* Paper link: https://ras.papercept.net/proceedings/IROS20/1216.pdf

### IROS Best Entertainment and Amusement Paper Award Sponsored by JTCF	
* Towards Micro Robot Hydrobatics: Vision-based Guidance, Navigation, and Control for Agile Underwater Vehicles in Confined Environments
* Authors: Duecker, Daniel Andre; Bauschmann, Nathalie; Hansen, Tim; Kreuzer, Edwin; and Seifried, Robert
* Paper link: https://ras.papercept.net/proceedings/IROS20/1220.pdf

### IROS Best Paper Award on Safety, Security, and Rescue Robotics in memory of Motohiro Kisoi sponsored by IRSI	
* Autonomous Spot: Long-Range Autonomous Exploration of Extreme Environments with Legged Locomotion
* Authors: Bouman, Amanda; Ginting, Muhammad Fadhil; Alatur, Nikhilesh; Palieri, Matteo; Fan, David D; Et al
* Paper link: https://ras.papercept.net/proceedings/IROS20/2314.pdf

### IROS Best RoboCup Paper Award Sponsored by RoboCup Federation	
* Real-Time Constrained Nonlinear Model Predictive Control on SO(3) for Dynamic Legged Locomotion
* Authors: Hong, Seungwoo; Kim, Joon-Ha; and Park, Hae-Won
* Paper link: https://ras.papercept.net/proceedings/IROS20/2325.pdf
